{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "limiting-cooling",
   "metadata": {},
   "source": [
    "# Lab 03 - Topic Modelling\n",
    "In this lab we will look into building topic models, but will also examine dimensionality reduction and other relevant subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-institute",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)\n",
    "Based on: [Text Mining 101: A Stepwise Introduction to Topic Modeling using Latent Semantic Analysis (using Python)](https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-teens",
   "metadata": {},
   "source": [
    "### Data reading and inspection\n",
    "Let’s load the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "static-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (1.22.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (4.29.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.11.2\n"
     ]
    }
   ],
   "source": [
    "import pip\n",
    "pip.main(['install','seaborn'])\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-console",
   "metadata": {},
   "source": [
    "We will use the ’20 Newsgroup’ dataset from sklearn.\n",
    "\n",
    "OPTIONALY: You can also download the dataset [here](https://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups), if you want to look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "restricted-corruption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-financing",
   "metadata": {},
   "source": [
    "Let's look at the 20 labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tracked-devices",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-estimate",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "To start with, we will try to clean our text data as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1d08b",
   "metadata": {},
   "source": [
    "### Challenge 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "measured-laser",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1002/1191655101.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "# TODO: Remove everything except alphabets\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "# TODO: Remove short words (words with len < 4) from `clean_doc` column.\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# TODO: Make all text lowercase\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-opportunity",
   "metadata": {},
   "source": [
    "Tokenise and remove the stop-words... eventually we will stitch the text back together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20496337",
   "metadata": {},
   "source": [
    "### Challenge 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "equal-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# TODO: Tokenization\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "# TODO: Remove stop-words\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [el for el in x if el not in stop_words])\n",
    "\n",
    "# de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "\n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-naples",
   "metadata": {},
   "source": [
    "### TFIDF Document-Term Matrix\n",
    "This is the first step towards topic modeling. We will use sklearn’s TfidfVectorizer to create a document-term matrix with 1,000 terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dec859",
   "metadata": {},
   "source": [
    "### Challenge 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "compressed-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "max_features= 1000, # keep top 1000 terms \n",
    "max_df = 0.5, \n",
    "smooth_idf=True)\n",
    "\n",
    "# TODO: Use fit_transform func to transform `clean_doc` with TfidfVectorizer\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "\n",
    "X.shape # check shape of the document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-ferry",
   "metadata": {},
   "source": [
    "We could have used all the terms to create this matrix but that would need quite a lot of computation time and resources. Hence, we have restricted the number of features to 1,000. If you have the computational power, I suggest trying out all the terms.\n",
    "\n",
    "### Topic Modeling\n",
    "The next step is to represent each and every term and document as a vector. We will use the document-term matrix and decompose it into multiple matrices. We will use sklearn’s TruncatedSVD to perform the task of matrix decomposition.\n",
    "\n",
    "Since the data comes from 20 different newsgroups, let’s try to have 20 topics for our text data. The number of topics can be specified by using the n_components parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "possible-break",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "# TODO: Fit svd_model on X\n",
    "svd_model.fit(X)\n",
    "\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-practitioner",
   "metadata": {},
   "source": [
    "The components of svd_model are our topics, and we can access them using svd_model.components_. Finally, let’s print a few most important words in each of the 20 topics and see how our model has done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "sustained-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: \n",
      "like  \n",
      "know  \n",
      "people  \n",
      "think  \n",
      "good  \n",
      "time  \n",
      "thanks  \n",
      "Topic 1: \n",
      "thanks  \n",
      "windows  \n",
      "card  \n",
      "drive  \n",
      "mail  \n",
      "file  \n",
      "advance  \n",
      "Topic 2: \n",
      "game  \n",
      "team  \n",
      "year  \n",
      "games  \n",
      "season  \n",
      "players  \n",
      "good  \n",
      "Topic 3: \n",
      "drive  \n",
      "scsi  \n",
      "disk  \n",
      "hard  \n",
      "card  \n",
      "drives  \n",
      "problem  \n",
      "Topic 4: \n",
      "windows  \n",
      "file  \n",
      "window  \n",
      "files  \n",
      "program  \n",
      "using  \n",
      "problem  \n",
      "Topic 5: \n",
      "government  \n",
      "chip  \n",
      "mail  \n",
      "space  \n",
      "information  \n",
      "encryption  \n",
      "data  \n",
      "Topic 6: \n",
      "like  \n",
      "bike  \n",
      "know  \n",
      "chip  \n",
      "sounds  \n",
      "looks  \n",
      "look  \n",
      "Topic 7: \n",
      "card  \n",
      "sale  \n",
      "video  \n",
      "offer  \n",
      "monitor  \n",
      "price  \n",
      "jesus  \n",
      "Topic 8: \n",
      "know  \n",
      "card  \n",
      "chip  \n",
      "video  \n",
      "government  \n",
      "people  \n",
      "clipper  \n",
      "Topic 9: \n",
      "good  \n",
      "know  \n",
      "time  \n",
      "bike  \n",
      "jesus  \n",
      "problem  \n",
      "work  \n",
      "Topic 10: \n",
      "think  \n",
      "chip  \n",
      "good  \n",
      "thanks  \n",
      "clipper  \n",
      "need  \n",
      "encryption  \n",
      "Topic 11: \n",
      "thanks  \n",
      "right  \n",
      "problem  \n",
      "good  \n",
      "bike  \n",
      "time  \n",
      "window  \n",
      "Topic 12: \n",
      "good  \n",
      "people  \n",
      "windows  \n",
      "know  \n",
      "file  \n",
      "sale  \n",
      "files  \n",
      "Topic 13: \n",
      "space  \n",
      "think  \n",
      "know  \n",
      "nasa  \n",
      "problem  \n",
      "year  \n",
      "israel  \n",
      "Topic 14: \n",
      "space  \n",
      "good  \n",
      "card  \n",
      "people  \n",
      "time  \n",
      "nasa  \n",
      "thanks  \n",
      "Topic 15: \n",
      "people  \n",
      "problem  \n",
      "window  \n",
      "time  \n",
      "game  \n",
      "want  \n",
      "bike  \n",
      "Topic 16: \n",
      "time  \n",
      "bike  \n",
      "right  \n",
      "windows  \n",
      "file  \n",
      "need  \n",
      "really  \n",
      "Topic 17: \n",
      "time  \n",
      "problem  \n",
      "file  \n",
      "think  \n",
      "israel  \n",
      "long  \n",
      "mail  \n",
      "Topic 18: \n",
      "file  \n",
      "need  \n",
      "card  \n",
      "files  \n",
      "problem  \n",
      "right  \n",
      "good  \n",
      "Topic 19: \n",
      "problem  \n",
      "file  \n",
      "thanks  \n",
      "used  \n",
      "space  \n",
      "chip  \n",
      "sale  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(svd_model.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0],\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-opportunity",
   "metadata": {},
   "source": [
    "### Topics Visualization\n",
    "To find out how distinct our topics are, we should visualize them. Of course, we cannot visualize more than 3 dimensions, but there are techniques like t-SNE which can help us visualize high dimensional data into lower dimensions. Here we will use a relatively new technique called UMAP (Uniform Manifold Approximation and Projection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "meaning-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting umap-learn\n",
      "  Using cached umap_learn-0.5.2-py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn) (1.8.0)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.6-py3-none-any.whl\n",
      "Collecting numba>=0.49\n",
      "  Using cached numba-0.55.1-1-cp39-cp39-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn) (1.22.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from umap-learn) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from numba>=0.49->umap-learn) (59.4.0)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Using cached llvmlite-0.38.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
      "Installing collected packages: numpy, llvmlite, numba, pynndescent, umap-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.2\n",
      "    Uninstalling numpy-1.22.2:\n",
      "      Successfully uninstalled numpy-1.22.2\n",
      "Successfully installed llvmlite-0.38.0 numba-0.55.1 numpy-1.21.5 pynndescent-0.5.6 umap-learn-0.5.2\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Numba needs NumPy 1.21 or less",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1002/1114172718.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pip install umap-learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'install'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'umap-learn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mumap_\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/umap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplefilter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mumap_\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUMAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/umap/umap_.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtril\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_tril\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriu\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_triu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mumap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistances\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0m_ensure_llvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# we know llvmlite is working as the above tests passed, import it now as SVML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/numba/__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Numba needs NumPy 1.18 or greater\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnumpy_version\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Numba needs NumPy 1.21 or less\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 1.21 or less"
     ]
    }
   ],
   "source": [
    "# pip install umap-learn\n",
    "pip.main(['install','umap-learn'])\n",
    "import umap.umap_ as umap\n",
    "\n",
    "X_topics = svd_model.fit_transform(X)\n",
    "embedding = umap.UMAP(n_neighbors=150, min_dist=0.5, random_state=12).fit_transform(X_topics)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], \n",
    "            c = dataset.target,\n",
    "            s = 10, # size\n",
    "            edgecolor='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1761fba3-2511-4fda-9f3b-494188ba15f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
