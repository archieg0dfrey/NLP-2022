{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "happy-sigma",
   "metadata": {},
   "source": [
    "# Lab 01 - Tokenisation and basic feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-supervision",
   "metadata": {},
   "source": [
    "This lab is an introduction to some basic test processing that will give us the first impression of the challenges in translating text into meaningful feature vector representations.\n",
    "\n",
    "So let's first set some text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-throat",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\" Welcome to the Natural Language Processing lab!\n",
    "               We'll learn many things in this no 1 lab, so we will take it easy.\n",
    "               Natural Language Processing is fun.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86574662",
   "metadata": {},
   "source": [
    "Let's also download some libraries we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76028587",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas sklearn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da2c66",
   "metadata": {},
   "source": [
    "## Using native Python functions for tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-perry",
   "metadata": {},
   "source": [
    "Let's try to split the text based on spaces using the built in string function `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-brain",
   "metadata": {},
   "source": [
    "You will observe that some words are not well separated from punctuation and contain some appended onto them.\n",
    "So we need to find a way to remove those characters... but, before we do that, let's see how we can create a quick feature vector first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sentence.split()  # splitting based on spaces\n",
    "vocab = sorted(set(tokens))  # sorting and removing duplicates by using set()\n",
    "vocab  # just printing the vocab so we can look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-syndication",
   "metadata": {},
   "source": [
    "We can see that the sorted list has the numbers first, followed by capital and then lower case letters (all alphabetically sorted). We also see that repeating words appear only once in our vocabulary list. Let's compare the size of the two lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_len = len(tokens)\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "print(f\"Tokens: {tokens_len}\")\n",
    "print(f\"Vocab: {vocab_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-private",
   "metadata": {},
   "source": [
    "Let's try and print the matrix of tokens against vocabulary. We will use the numpy lib for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "matrix = np.zeros((tokens_len, vocab_len), int)\n",
    "for i, token in enumerate(tokens):\n",
    "    matrix[i, vocab.index(token)] = 1\n",
    "\n",
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-beach",
   "metadata": {},
   "source": [
    "It's not easy to see, but the second, third and fourth columns have the value of 1 in two rows, whereas the rest only take the value of 1 in a single row. To make it a little more readable, we could use Pandas and DataFrame! Both Pandas and NumPy are very useful libs that we will use many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-wallace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(matrix, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increasing-barrel",
   "metadata": {},
   "source": [
    "Now this is a lot more clear and if we wanted we could carry on making it look nicer.\n",
    "\n",
    "Let's now carry on building the bag of words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = {token: 1 for token in tokens}  # setting this up as a dictionary\n",
    "sorted(bow.items())  # lets print it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-palestine",
   "metadata": {},
   "source": [
    "Since bow is a dictionary, we see that there are no duplicate words.\n",
    "\n",
    "Pandas also has a more efficient form of a dictionary called `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-aquatic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.Series(dict([(token, 1) for token in tokens])), columns=[\"sent\"]).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus[f\"sent{i}\"] = dict((tok, 1) for tok in sent.split())\n",
    "\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-rachel",
   "metadata": {},
   "source": [
    "Now we see how we managed to build feature vectors for the three sentences we originally had. Now let's do a Dot Product calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-tyler",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.T\n",
    "print(f\"Dot product of sent0 from sent1: {df.sent0.dot(df.sent1)} and dot product of sent0 from sent1: {df.sent0.dot(df.sent2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-pavilion",
   "metadata": {},
   "source": [
    "As we see from the results, the higher the dot product to more similar the vectors are... so given that only the first and last sentence have some common words, we see that this comes back as 3, where as the two sentences who have nothing in common come bak as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-realtor",
   "metadata": {},
   "source": [
    "We can improve our vocabulary now if we were to remove all other punctuation. Let's do that with regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-fifty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = re.split(r\"[-\\s.,;!?]+\", sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710aa204",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "Although this seems to be great... you might still have issues with different characters that are not anticipated. So we usually use an existing NLP related tokeniser to do this job. Let's try the NLTK lib.\n",
    "\n",
    "NLTK also supports regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+|$[0-9.]+|\\S+\")\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-prototype",
   "metadata": {},
   "source": [
    "but there are other more specialised tokenisers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-graphic",
   "metadata": {},
   "source": [
    "For now let's use the regular expression special word pattern `\\w`, so we can control what the tokeniser does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-category",
   "metadata": {},
   "source": [
    "At the point you could try out different other tokenisers from other libraries and see if there are any differences.\n",
    "\n",
    "We will now calculate the 2-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "list(ngrams(tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-present",
   "metadata": {},
   "source": [
    "and 3-grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(tokens, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-shooting",
   "metadata": {},
   "source": [
    "If we want to include the n-grams as strings rather than tuples, then we need to convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [\" \".join(x) for x in list(ngrams(tokens, 2))]\n",
    "print(f\"Bigrams: {bigrams}\\n\")\n",
    "\n",
    "trigrams = [\" \".join(x) for x in list(ngrams(tokens, 3))]\n",
    "print(f\"Trigrams: {trigrams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-syracuse",
   "metadata": {},
   "source": [
    "Another important step we looked at in the lectures are the stop words. Let's try to use the nltk stop word list to remove them.\n",
    "\n",
    "First, let's download the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-disorder",
   "metadata": {},
   "source": [
    "and now check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(f\"number of stopwords: {len(stop_words)}\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-expression",
   "metadata": {},
   "source": [
    "Other libs have different stopwords. Let's see a much larger set from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words\n",
    "\n",
    "print(f\"number of stopwords: {len(sklearn_stop_words)}\")\n",
    "print(sklearn_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exempt-deviation",
   "metadata": {},
   "source": [
    "Strangely enough, although there are more stop words in sklearn, you will find that nltk has words that are not contained in sklearn. So you might want to join the two lists.\n",
    "\n",
    "For normalising the text you could do something as simple as making sure all words are lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tokens = [x.lower() for x in tokens]\n",
    "print(norm_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-inflation",
   "metadata": {},
   "source": [
    "For stemming the words we could use nltk again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-reputation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem_tokens = [stemmer.stem(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-porter",
   "metadata": {},
   "source": [
    "For lemmatising, nltk also does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stem_tokens = [lemmatizer.lemmatize(x) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-relevance",
   "metadata": {},
   "source": [
    "With this example sentence, there are no issues with the lemmatisation... but let's look at the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\", \"a\"))  # declaring the POS as adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-martin",
   "metadata": {},
   "source": [
    "If we don't include the Part-of-speech (POS), nltk, using wordnet, does not work well. So let's try to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-merit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map the POS tag to the first character lemmatize() accepts.\"\"\"\n",
    "\n",
    "    try:  # download nltk's POS tagger if it doesn't exist\n",
    "        nltk.data.find(\"taggers/averaged_perceptron_tagger\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"averaged_perceptron_tagger\")\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()  # use ntlk's POS tagger on the word\n",
    "\n",
    "    # now we need to convert from nltk to wordnet POS notations (for compatibility reasons)\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # return and default to noun if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stem_tokens = [lemmatizer.lemmatize(x, pos=get_wordnet_pos(x)) for x in norm_tokens]\n",
    "print(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-assignment",
   "metadata": {},
   "source": [
    "If we look at the words now we are getting more counts for our bag of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polished-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bow = Counter(stem_tokens)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-healthcare",
   "metadata": {},
   "source": [
    "Now let's check the most frequent 6 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow.most_common(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-hayes",
   "metadata": {},
   "source": [
    "Now let's remove the stop words and check the count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_tokens = [x for x in stem_tokens if x not in stop_words]\n",
    "count = Counter(no_stop_tokens)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-favor",
   "metadata": {},
   "source": [
    "Finally... let's make our feature vector using the frequency ratio (term count / total number of terms in the doc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(no_stop_tokens)\n",
    "for key, value in count.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "\n",
    "print(document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-spell",
   "metadata": {},
   "source": [
    "We have explored many many options already and we will continue with more advanced feature vectors in the next lab, plus some visualisations in charts. So until then please try different experiments on your own:\n",
    "1. See if you can change the text and have more sentences with different topics (so you can compare the feature vectors later)\n",
    "2. Try to use different libraries for tokenising, PoS tagging, stemming and lemmatising.\n",
    "3. Try to use distance metrics to compare vectors, such as Euclidian distance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
