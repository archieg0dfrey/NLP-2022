{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "graduate-florida",
   "metadata": {},
   "source": [
    "# Lab 02 - Feature Vectors and Language Models\n",
    "In this lab we will continue working with words as features, but the focus will be to build language models.\n",
    "\n",
    "First, let's make sure the libraries we'll be using are installed and initialized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30409a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas nltk matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd7019d",
   "metadata": {},
   "source": [
    "\n",
    "## Words as Feature Vectors\n",
    "First, let's look at some text collections directly available in [NLTK](https://www.nltk.org).\n",
    "\n",
    "To explore all the available Corpora in NLTK we just need to run a command and list the available resources, after we have downloaded them. The documentation on NLTK's [website](https://www.nltk.org/book/ch02.html) will give you more details on each Corpus.\n",
    "\n",
    "There are many interesting collections such as: the Gutenberg collection of books, the  Brown collection of news, novels and other stories, and the USA presidential inaugural speeches.\n",
    "\n",
    "Let's start by downloading the resources. NLTK's `download()` command will launch a GUI, or a CLI to let you select the data you want to install.\n",
    "\n",
    "**Use it to download the `inaugural` Corpus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-morris",
   "metadata": {},
   "source": [
    "Now we should be able to list the downloaded resources as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-casino",
   "metadata": {},
   "source": [
    "So to start with, let's experiment with the USA Presidential inaugural speeches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural\n",
    "print(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-beginning",
   "metadata": {},
   "source": [
    "Lets print Trump's one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-boards",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"2017-Trump.txt\"\n",
    "print(inaugural.raw(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-colonial",
   "metadata": {},
   "source": [
    "NLTK corpora gives us all the words and sentences as well as other statistics out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.words(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\")\n",
    "print(inaugural.sents(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-caribbean",
   "metadata": {},
   "source": [
    "## Challenge 01\n",
    "Given this incomplete function, write the necessary `TODO X` code to let the function return the total number of words and the total number of distinct words, for a given document name in that corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inaugural_stats(doc):\n",
    "    # TODO 1 - Get the pre-tokenised list of words from the inaugural corpus\n",
    "    doc_words = ...\n",
    "\n",
    "    # TODO 2 - Calculate the total number of words\n",
    "    num_words = ...\n",
    "\n",
    "    # TODO 3 - Calculate the total number of distinct words (vocabulary)\n",
    "    vocab = ...\n",
    "\n",
    "    return num_words, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-seller",
   "metadata": {},
   "source": [
    "Now let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inaugural_stats(speech_name):\n",
    "    tokens, vocab = calculate_inaugural_stats(speech_name)\n",
    "    print(f\"Num words in {speech_name}: {tokens}\")\n",
    "    print(f\"Vocabulary size: {vocab}\")\n",
    "\n",
    "print_inaugural_stats(\"2017-Trump.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-while",
   "metadata": {},
   "source": [
    "Let's compare Trump's speech against Obama's inaugural speech..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-large",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_inaugural_stats(\"2009-Obama.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-liberal",
   "metadata": {},
   "source": [
    "## Challenge 02\n",
    "Complete the missing code (`TODO X`) in the function to calculate the average word length (i.e. number of characters per word) of a given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_inaugural_word_stats(doc):\n",
    "    doc_words = inaugural.words(doc)\n",
    "\n",
    "    # TODO 1 - Construct a list that contains the word lengths for each DISTINCT word in the document\n",
    "    vocab_lengths = ...\n",
    "\n",
    "    # TODO 2 - Find the average word type length\n",
    "    avg_vocab_length = ...\n",
    "\n",
    "    return avg_vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-salem",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_name = \"2017-Trump.txt\"\n",
    "avg_length = calculate_inaugural_word_stats(speech_name)\n",
    "print(f\"Average word length for {speech_name}: {avg_length:.2f} characters long\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blond-melissa",
   "metadata": {},
   "source": [
    "Now it will be interesting to look at the word distribution and see how the last two USA Presidents compare. NLTK again has a nice class with functions for that, `FreqDist`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "obama_words = inaugural.words(\"2009-Obama.txt\")\n",
    "trump_words = inaugural.words(\"2017-Trump.txt\")\n",
    "\n",
    "# Construct a frequency distribution over the lowercased words in the document\n",
    "fd_obama = FreqDist(w.lower() for w in obama_words)\n",
    "# Find the top 50 most frequently used words in the speech\n",
    "print(\"\\nOBAMA\\n\", fd_obama.most_common(50))\n",
    "\n",
    "# Construct a frequency distribution over the lowercased words in the document\n",
    "fd_trump = FreqDist(w.lower() for w in trump_words)\n",
    "# Find the top 50 most frequently used words in the speech\n",
    "print(\"\\nTRUMP\\n\", fd_trump.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-trouble",
   "metadata": {},
   "source": [
    "As you might have expected... popular words in Trump's speech are: `will`, `america`, `american`, `people`, `country`, `again`... :-)\n",
    "\n",
    "Now let's plot the distributions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd_obama.plot(50)\n",
    "fd_trump.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-painting",
   "metadata": {},
   "source": [
    "Those distributions are \"normal\" and most documents or corpora will follow a very similar curve.\n",
    "\n",
    "Let's compare some word frequencies now between the two Presidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Obama -> peace: {fd_obama['peace']} - america: {fd_obama['america']}\")\n",
    "print(f\"Trump -> peace: {fd_trump['peace']} - america: {fd_trump['america']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-scholar",
   "metadata": {},
   "source": [
    "## Challenge 03\n",
    "Let's try to build a similar function that calculates the top most frequent words in a document, but using the `Counter` class that we used in the previous lab. Complete the `TODO X` sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brown-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_top_freq(doc, k=50):\n",
    "    doc_words = inaugural.words(doc)\n",
    "    \n",
    "    # TODO 1 - Construct a frequency distribution over the words in the document, ensuring all words are lowercase\n",
    "    fd_doc_words = ...\n",
    "    \n",
    "    # TODO 2 - Find the top x most frequently used words in the document\n",
    "    top_words = ...\n",
    "\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-market",
   "metadata": {},
   "source": [
    "Now let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top 50 words for Trump's 2017 speech:\\n{get_top_freq('2017-Trump.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-slide",
   "metadata": {},
   "source": [
    "## Challenge 04\n",
    "Now let's try to build a TFIDF feature vector!\n",
    "\n",
    "The first thing is to calculate the Term Frequency (TF).\n",
    "\n",
    "There are different ways to calculate the Term Frequency. Try to implement the formula $tf_{t,d}=\\frac{count(t,d)}{count(d)}$ that we used before in the lecture. Complete the `TODO X` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf(token_count, bow):\n",
    "    tf = {}\n",
    "    num_bow = len(bow)\n",
    "\n",
    "    for token, count in token_count.items():\n",
    "        # TODO - Calculate the term frequency using the formula:\n",
    "        # \"count of term in the document\" / \"total number of words in the document\"\n",
    "        tf[token] = ...\n",
    "\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-quest",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_01 = inaugural.words(\"2017-Trump.txt\")\n",
    "tokens_02 = inaugural.words(\"2009-Obama.txt\")\n",
    "vocab = set(tokens_01).union(set(tokens_02))\n",
    "\n",
    "def _get_tf(tokens, vocab):\n",
    "    token_count = dict.fromkeys(vocab, 0)\n",
    "    for token in tokens:\n",
    "        token_count[token] += 1\n",
    "    return calculate_tf(token_count, tokens)\n",
    "\n",
    "tf_01 = _get_tf(tokens_01, vocab)\n",
    "tf_02 = _get_tf(tokens_02, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-european",
   "metadata": {},
   "source": [
    "## Challenge 05\n",
    "Now that we have our term frequency, let's calculate the Inverse Term Frequency (IDF) for a list of documents.\n",
    "\n",
    "We are going to use the original formula here: $\\log{\\frac{N}{n_t}}$, where $N$ is the number of documents and $n_t$ is the number of documents that contain the term $t$. Complete the `TODO X` sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_idf(docs):\n",
    "    N = len(docs)\n",
    "    \n",
    "    # TODO 1 - Initialise a new dictionary with the keys from the documents and the values set to 0\n",
    "    idf = ...\n",
    "    for doc in docs:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                # TODO 2 - Increase the idf dictionary counter by one\n",
    "                ...\n",
    "    \n",
    "    for word, val in idf.items():\n",
    "        idf[word] = math.log(N / float(val))\n",
    "\n",
    "    return idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-textbook",
   "metadata": {},
   "source": [
    "Now let's collect all the IDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [tf_01, tf_02]\n",
    "idfs = calculate_idf(tfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-child",
   "metadata": {},
   "source": [
    "## Challenge 06\n",
    "And finally... the TFIDF calculation.\n",
    "\n",
    "Calculate the TFIDF for all the documents. Complete the `TODO X` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-superintendent",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_tfidf(tfs, idfs):\n",
    "    tfidf = {}\n",
    "\n",
    "    for word, val in tfs.items():\n",
    "        # TODO - Calculate and store the tfidf\n",
    "        ...\n",
    "\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-inspection",
   "metadata": {},
   "source": [
    "Let's test test the TFIDFs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_01 = calculate_tfidf(tf_01, idfs)\n",
    "tfidf_02 = calculate_tfidf(tf_02, idfs)\n",
    "print(f\"Trump: {tfidf_01}\")\n",
    "print(f\"\\nObama: {tfidf_02}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-performer",
   "metadata": {},
   "source": [
    "## Challenge 07\n",
    "Let's try it and then visualise it as a DataFrame!\n",
    "\n",
    "Try to fit the two dictionaries into a single DataFrame so that we can visualise it better. Complete the `TODO X` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TODO\n",
    "df = ...\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-muscle",
   "metadata": {},
   "source": [
    "Now we have TFIDF feature vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-bouquet",
   "metadata": {},
   "source": [
    "# Language Model Experiments\n",
    "Let's play with some popular n-gram language models. We are going to use NLTK again for that and try to predict the sequence of words.\n",
    "\n",
    "First, let's import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.lm import WittenBellInterpolated, MLE, Laplace\n",
    "from nltk.util import ngrams, pad_sequence, everygrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-insulin",
   "metadata": {},
   "source": [
    "Next we will build a function that performs the prediction based on MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mle_estimator(doc_name, n):\n",
    "    # Construct a list of lowercase words from the document\n",
    "    words = [w.lower() for w in inaugural.words(doc_name)]\n",
    "    \n",
    "    # generate ngrams\n",
    "    ngrams = list(everygrams(words, max_len=n))\n",
    "\n",
    "    # build ngram language models\n",
    "    lm = MLE(n)\n",
    "    lm.fit([ngrams], vocabulary_text=words)\n",
    "    print(lm.vocab)\n",
    "    \n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-thriller",
   "metadata": {},
   "source": [
    "Build the estimator first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = build_mle_estimator(\"2017-Trump.txt\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-stevens",
   "metadata": {},
   "source": [
    "Now let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-charlotte",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lm_scores(lm):\n",
    "    print(f\"Probability of 'first' followed by 'america': {lm.score(word='america', context=['first']):.5f}\")\n",
    "    print(f\"Probability of 'america' followed by 'first': {lm.score(word='first', context=['america']):.5f}\")\n",
    "\n",
    "    print(f\"Probability of 'you' followed by 'thank': {lm.score(word='thank', context=['you']):.5f}\")\n",
    "    print(f\"Probability of 'thank' followed by 'you': {lm.score(word='you', context=['thank']):.5f}\")\n",
    "\n",
    "print_lm_scores(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-passion",
   "metadata": {},
   "source": [
    "## Challenge 08\n",
    "Try an add-one Laplace smoothing model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_laplace_estimator(doc_name, n):\n",
    "    # TODO - Implement a function simlar to `build_mle_estimator` that instead uses an add-one Laplace smoothing model.\n",
    "    # Hint: you might want to check the NLTK documentation (https://www.nltk.org/api/nltk.lm.html) on that!\n",
    "    ...\n",
    "\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-board",
   "metadata": {},
   "source": [
    "Build this estimator as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = build_laplace_estimator('2017-Trump.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-dining",
   "metadata": {},
   "source": [
    "Now test it and observe any differences in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_lm_scores(lm2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
