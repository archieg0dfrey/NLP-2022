{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "specific-hierarchy",
   "metadata": {},
   "source": [
    "# Lab 05 - Word Embeddings\n",
    "In this lab we will look into word embeddings with word2vec and other similar methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-money",
   "metadata": {},
   "source": [
    "### Build your own\n",
    "Let's start by first building out own word2vec model, instead of downloading a ready trained one. For that we are going to use the 20 news groups from sklearn, since is not too large for a lab exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-maintenance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "\n",
    "# lets check the first two documents\n",
    "documents[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-internship",
   "metadata": {},
   "source": [
    "The first thing to do is to format the documents into a list of sentences that contains a list of tokens. We are not going to do any further cleaning and pre-processing for now (to keep things simple for the labs), but that would be advisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# This will take a minute or so\n",
    "token_list = []\n",
    "for d in documents:\n",
    "    s = sent_tokenize(d) \n",
    "    token_list = token_list +[word_tokenize(t) for t in s]\n",
    "\n",
    "# check the first three sentences\n",
    "token_list[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-bahamas",
   "metadata": {},
   "source": [
    "Now is time to import the word2vec algorithm and set the key parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-grant",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Number of vector elements (dimensions) to represent the word vector\n",
    "num_features = 300\n",
    "# Min number of word count to be considered in the Word2vec model. If your corpus is small, reduce the min count. If you’re training with a large corpus, increase the min count.\n",
    "min_word_count = 1\n",
    "# Number of CPU cores used for the training. If you want to set the number of cores dynamically, check out import multiprocessing: \n",
    "#num_workers = multiprocessing.cpu_count()\n",
    "num_workers = 2\n",
    "# Context window size\n",
    "window_size = 3\n",
    "# Subsampling rate for frequent terms\n",
    "subsampling = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-correlation",
   "metadata": {},
   "source": [
    "Let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(token_list, workers=num_workers, vector_size=num_features, min_count=min_word_count, window=window_size, sample=subsampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-initial",
   "metadata": {},
   "source": [
    "Once you’ve trained your word model, you can reduce the memory footprint by about half if you freeze your model and discard the unnecessary information. The following command will discard the unneeded output weights of your neural network:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-longer",
   "metadata": {},
   "source": [
    "The model cannot be trained further once the weights of the output layer have been discarded.\n",
    "\n",
    "Save the trained model with the following command and preserve it for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"my_own_domain_specific_word2vec_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-prize",
   "metadata": {},
   "source": [
    "Now lets say we want to load the model that we had previously saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-trauma",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model_name = \"my_own_domain_specific_word2vec_model\"\n",
    "model = Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-winning",
   "metadata": {},
   "source": [
    "Let's check the most similar words to \"justice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar('justice'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-backup",
   "metadata": {},
   "source": [
    "### Challenge - 1\n",
    "Try a few more words and observe if what is retrieved makes sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-detector",
   "metadata": {},
   "source": [
    "### Using the gensim API\n",
    "Having build our own model is great, but lets now load a model that was trained with MANY documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-difference",
   "metadata": {},
   "source": [
    "We will be using the downloader for the embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# this command can be used to check what models are available\n",
    "#api.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-increase",
   "metadata": {},
   "source": [
    "Let's load the word2vec model from google news containing 300 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will also take a minute or so\n",
    "word2vec_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-outline",
   "metadata": {},
   "source": [
    "Now check the embedding vector for \"beautiful\"... you will see a 300 dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model[\"beautiful\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-projection",
   "metadata": {},
   "source": [
    "Let's check some similar words to the word \"girl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-collapse",
   "metadata": {},
   "source": [
    "How about some maths with vectors! Try the following:\n",
    "\n",
    "queen - girl + boy = king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.most_similar(positive=['boy', 'queen'], negative=['girl'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial-indication",
   "metadata": {},
   "source": [
    "Time to do some visualisations and see how similar words end up close together and far from other words that are not as similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    wordvecs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "    coordinates = tsne_model.fit_transform(wordvecs)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-banking",
   "metadata": {},
   "source": [
    "### Challenge - 2\n",
    "Try a few more examples to visualise and see if similar words land close together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-limit",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "Let's try another model (GloVe) and see if that is any different to word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precise-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model[\"beautiful\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-archive",
   "metadata": {},
   "source": [
    "It will be interesting to see if this will fins similar words to \"girl\" like word2vec did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(\"girl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-custom",
   "metadata": {},
   "source": [
    "Let's also see if it can solve the same analogy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar(positive=['boy', 'queen'], negative=['girl'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\"boy\", \"girl\", \"man\", \"woman\", \"king\", \"queen\", \"banana\", \"apple\", \"mango\", \"fruit\", \"coconut\", \"orange\"]\n",
    "\n",
    "def tsne_plot(model):\n",
    "    labels = []\n",
    "    wordvecs = []\n",
    "\n",
    "    for word in vocab:\n",
    "        wordvecs.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=3, n_components=2, init='pca', random_state=42)\n",
    "    coordinates = tsne_model.fit_transform(wordvecs)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in coordinates:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(8,8)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(2, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "tsne_plot(glove_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-testimony",
   "metadata": {},
   "source": [
    "Let's continue with GloVe and check if plural words play any role in how close is to the original singular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(glove_model.distance(\"fruit\", \"fruits\"))\n",
    "print(glove_model.distance(\"girl\", \"girls\"))\n",
    "print(glove_model.distance(\"girl\", \"boy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-spirituality",
   "metadata": {},
   "source": [
    "## Challenge - 3\n",
    "Calculate the distance for \"king\" and \"queen\", then for \"woman and \"man\". Is it similar? Check the plot to confirm.\n",
    "\n",
    "Calculate the distance for \"king\" and \"apple\", then for \"queen\" and \"apple\". Is it similar again? Check the plot to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-bangkok",
   "metadata": {},
   "source": [
    "Now let's try and see if the model can find the capitals of different countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# pretty print function\n",
    "def pp(obj):\n",
    "    print(pd.DataFrame(obj))\n",
    "    \n",
    "def analogy(worda, wordb, wordc):\n",
    "    result = glove_model.most_similar(negative=[worda], positive=[wordb, wordc], topn=1)\n",
    "    return result[0][0]\n",
    "\n",
    "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
    "capitals = [analogy('usa', 'washington', country) for country in countries]\n",
    "pp(zip(countries,capitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-healthcare",
   "metadata": {},
   "source": [
    "## Challenge - 4\n",
    "Looks good... but what if you change \"usa\" to \"us\"? Or if you used a different example to start with like \"greece\" and \"athens\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-middle",
   "metadata": {},
   "source": [
    "Now let's plot the results on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_data(orig_data, labels):\n",
    "    pca = PCA(n_components=2)\n",
    "    data = pca.fit_transform(orig_data)\n",
    "    plt.figure(figsize=(7, 5), dpi=100)\n",
    "    plt.plot(data[:,0], data[:,1], '.')\n",
    "    for i in range(len(data)):\n",
    "        plt.annotate(labels[i], xy = data[i])\n",
    "    for i in range(len(data)//2):\n",
    "        plt.annotate(\"\",\n",
    "                xy=data[i],\n",
    "                xytext=data[i+len(data)//2],\n",
    "                arrowprops=dict(arrowstyle=\"->\",\n",
    "                                connectionstyle=\"arc3\")\n",
    "        )\n",
    "       \n",
    "labels = countries + capitals\n",
    "data = [glove_model[w] for w in labels]\n",
    "plot_data(data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-pavilion",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "Now let's look into generating feature vectors for documents instead of just words. For that we are going to use word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporate-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument,Doc2Vec\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-badge",
   "metadata": {},
   "source": [
    "First let's load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dataset.data\n",
    "\n",
    "training_corpus = []\n",
    "for i, text in enumerate(corpus):\n",
    "    tagged_doc = TaggedDocument(simple_preprocess(text), [i])\n",
    "    training_corpus.append(tagged_doc)\n",
    "    \n",
    "# If you’re running low on RAM, and you know the number of documents ahead of time (your corpus object isn’t an iterator or generator),\n",
    "# you might want to use a preallocated numpy array instead of Python list for your training_corpus:\n",
    "#training_corpus = np.empty(len(corpus), dtype=object);\n",
    "#… \n",
    "#training_corpus[i] = …"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-stereo",
   "metadata": {},
   "source": [
    "Now we will build the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model = Doc2Vec(vector_size=100, min_count=2, workers=num_cores, epochs=10)\n",
    "doc2vec_model.build_vocab(training_corpus)\n",
    "doc2vec_model.train(training_corpus, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37532adc-58b8-41ac-b3aa-4f410413daf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_2 = Doc2Vec(vector_size=300, min_count=2, workers=num_cores, epochs=10)\n",
    "doc2vec_model_2.build_vocab(training_corpus)\n",
    "doc2vec_model_2.train(training_corpus, total_examples=doc2vec_model_2.corpus_count, epochs=doc2vec_model_2.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da722aae-4e0e-4407-af79-df0bca685d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_3 = Doc2Vec(vector_size=100, min_count=2, workers=num_cores, epochs=20)\n",
    "doc2vec_model_3.build_vocab(training_corpus)\n",
    "doc2vec_model_3.train(training_corpus, total_examples=doc2vec_model_3.corpus_count, epochs=doc2vec_model_3.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208f0d8-3c1a-4fa0-b21e-718b96622a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_4 = Doc2Vec(vector_size=300, min_count=2, workers=num_cores, epochs=20)\n",
    "doc2vec_model_4.build_vocab(training_corpus)\n",
    "doc2vec_model_4.train(training_corpus, total_examples=doc2vec_model_4.corpus_count, epochs=doc2vec_model_4.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6883dc5-a6cb-40f8-80e4-f10ae2b5c22e",
   "metadata": {},
   "source": [
    "Time to generate the feature vector of a new document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb28986-388e-4d79-8fa4-2be002a9ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=10)\n",
    "doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627a77f5-54ec-47c0-8e6c-249dc2bc8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_2.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=10)\n",
    "doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6e468-c2e3-4ae4-a139-b6846872690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_3.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=20) # Use the increased number of epochs\n",
    "doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050c723c-2ba9-4838-9edf-3eea1d2bafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model_4.infer_vector(simple_preprocess('This is a completely unseen document'), epochs=20) # Use the increased number of epochs\n",
    "doc_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-prayer",
   "metadata": {},
   "source": [
    "Time to generate the feature vector of a new document! In this instance I just chose an existing one (i.e. the first one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vector = doc2vec_model.infer_vector(simple_preprocess(dataset.data[0]), epochs=10)\n",
    "doc_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-repair",
   "metadata": {},
   "source": [
    "I will now try to find the most similar documents. This will return the index of the most similar docs. Since I used an existing document as in input, I am expecting this (i.e. index 0) to be the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-hypothesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.docvecs.most_similar([doc_vector])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-party",
   "metadata": {},
   "source": [
    "Now lets check the two most similar documents and see if there is similarity in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" *** Doc 1 ***\")\n",
    "print(dataset.data[0])\n",
    "print(\" *** Doc 2 ***\")\n",
    "print(dataset.data[3580])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-talent",
   "metadata": {},
   "source": [
    "## Challenge - 5\n",
    "Use the fetch_20newsgroups dataset from sklearn (see code above) and re-train doc2vec with that data instead.\n",
    "\n",
    "Then, check using the most similar function to see if the documents you test are indeed similar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
